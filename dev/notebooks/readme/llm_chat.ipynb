{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cell_0",
      "metadata": {},
      "source": [
        "# Using LLMs for chat (text + vision), with fallback LLM support"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_1",
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -U contextgem"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_2",
      "metadata": {},
      "source": [
        "To run the extraction, please provide your LLM details in the ``DocumentLLM(...)`` constructor further below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Using LLMs for chat (text + vision), with fallback LLM support\n",
        "\n",
        "import os\n",
        "\n",
        "from contextgem import DocumentLLM\n",
        "from contextgem.public import ChatSession\n",
        "\n",
        "\n",
        "# Initialize main LLM for chat\n",
        "main_model = DocumentLLM(\n",
        "    model=\"openai/gpt-4o\",  # or another provider/model\n",
        "    api_key=os.getenv(\"CONTEXTGEM_OPENAI_API_KEY\"),  # your API key for the LLM provider\n",
        "    system_message=\"\",  # disable default system message for chat, or provide your own\n",
        ")\n",
        "\n",
        "# Optional: configure fallback LLM for reliability\n",
        "fallback_model = DocumentLLM(\n",
        "    model=\"openai/gpt-4o-mini\",  # or another provider/model\n",
        "    api_key=os.getenv(\"CONTEXTGEM_OPENAI_API_KEY\"),  # your API key for the LLM provider\n",
        "    is_fallback=True,\n",
        "    system_message=\"\",  # also disable default system message for fallback, or provide your own\n",
        ")\n",
        "main_model.fallback_llm = fallback_model\n",
        "\n",
        "\n",
        "# Preserve conversation history across turns with a ChatSession\n",
        "session = ChatSession()\n",
        "first_response = main_model.chat(\n",
        "    \"Hi there!\",\n",
        "    # images=[Image(...)]  # optional: add images for vision models\n",
        "    chat_session=session,\n",
        ")\n",
        "second_response = main_model.chat(\n",
        "    \"And what is EBITDA?\",\n",
        "    chat_session=session,\n",
        ")\n",
        "# or use async: `response = await main_model.chat_async(...)`\n",
        "\n",
        "# Or send a chat message without a session (one-off message \u2192 response)\n",
        "one_off_response = main_model.chat(\"Test\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}